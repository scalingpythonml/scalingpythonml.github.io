= Running Spark Jupyter Notebooks Client Mode inside of a Kubernetes Cluster (with ARM for Extra Fun)


Having your Spark Notebook inside the same cluster as the executors can reduce network errors and improve uptime. Since these network issues can result in job failure, this is an important consideration. This post assumes that you've already set up the foundation JupyterHub inside of Kubernetes deployment; the Dask-distributed notebook blog post covers that if you haven't.

To get a Spark notebook working inside of the cluster, we need to set up a few different things. The first step, similar to dask-kubernetes, is building a container with Jupyter and Spark installed. We also need to make a container of Spark for the executors. In addition to the containers, we need to set up permissions on the cluster and ensure that the executors that your Spark driver will launch have a way to talk to the driver in the notebook.

[NOTE]
====
It may seem like there are extra steps here compared to dask-kubernetes. Dask-kubernetes automates some service creation, which allows for communication between the scheduler, executors, and the notebook.
====

== Building the Containers


We need two containers, one with Jupyter and Spark installed together and another with just Spark. Since we're working in Python, there are some extra Python libraries we want to install as well (PyArrow, pandas, etc.) If you've got a specific version of a library that your project depends on, you'll want to add it to both the Jupyter Spark driver container and the executor containers.


To start with we'll download link:$$http://spark.apache.org/$$[Apache Spark] and decompress it, as shown in <<dlspark>>, so that we can copy the desired parts inside our containers.

.Download Spark
[[dlspark]]
====
include::../subrepos/scalingpythonml/spark/containers/[tags=dlspark]
====

Now that we have Spark downloaded we can start customizing our `Dockerfiles`.

=== Building the Jupyter Spark Container


The easiest way to build a Jupyter Spark container is to install Spark on top of the base Jupyter container. If you're running on ARM, you'll need to first cross-build the base Jupyter container (see my link:$$https://scalingpythonml.com/2020/12/12/deploying-jupyter-lab-notebook-for-dask-on-arm-on-k8s.html$$[instructions in the previous post]).


In my case I've custom built the link:$$https://github.com/jupyterhub/zero-to-jupyterhub-k8s/tree/master/images/singleuser-sample$$[single-user sample Docker container] from zero-to-jupyterhub-k8s to `holdenk/jupyter-hub-magicsingleuser-sample:0.10.2` as I needed ARM support. If you don't need to cross-build your custom container, you can use the pre-built container at `jupyterhub/k8s-singleuser-sample` as the basis for yours.


Since Spark needs Java to run, I decided to look at the link:$$https://github.com/docker-library/openjdk/blob/master/11/jdk/slim-buster/Dockerfile$$[jdk11 slim dockerfile] to see how to install Java in a dockerfile well. If you're an object-oriented person, you might be wishing we had multiple-inheritence with Dockerfiles, but that doesn't work. In addition to the JDK11 dockerfile, I looked at Spark's own Dockerfiles (includign PySpark) and the resulting Juptyer Spark Container specification is shown in <<spark_notebook_dockerfile>>.

.Dockerfile to add Spark on top of the Jupyter Notebook container.
[[spark_notebook_dockerfile]]
====
[source, dockerfile]
----
include::../subrepos/scalingpythonml/spark/containers/notebook/Dockerfile[]
----
====

Since the Dockerfile copies parts of Spark in, remember to save it at the root of where you decompressed the Spark tarball.


If you're not cross building, you can build this with a regular `docker build`, in my case since I'm targetting arm and x86 I did built it as shown in <<build_spark_nb>>.

.Build Spark notebook container
[[build_spark_nb]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/build.sh[tags=build-notebook]
----
====


[NOTE]
====
An alternative would have been to take the JDK-11 containers as a starting point, and install Jupyter on top of it, but when I tried that I found it more complicated.
====

This gives us a container with both Spark and the base notebook layer together. For the executors, we don't want to bother shipping Jupyter, so we'll build a seperate container for the executors.

=== Building the Executor Container


Spark does not ship pre-built containers for its executors, so regardless of which arch youâ€™re using, you will need to build the executor containers.


If you're building multi-arch containers, you will need to update Spark's docker image tool. You will need to change the buildx option to push the images by adding "--push" to the docker buildx commands in the script for ./bin/docker-image-tool.sh.



Spark's Python container Dockerfile installs an older version of Python without any dependencies, so you will want to customize your Python container setup, as well. My Dockerfile is shown in <<spark_exec_dockerfile>>.



.Dockerfile customizing PySpark setup
[[spark_exec_dockerfile]]
====
[source, dockerfile]
----
include::../subrepos/scalingpythonml/spark/containers/python-executor/Dockerfile[]
----
====

You'll see this file references `pysetup.sh` which installs Python using Miniforge so we can support arm as shown in <<pysetupsh>>.

.Setup python
[[spark_exec_dockerfile]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/pysetup.sh[]
----
====

You will want to make your Dockerfile install the dependencies for your program while making sure to select the same version of Python that you have in your Jupyter container, so you may need to modify those two examples.


Once you've configured your enviroment, you can build your Spark image using the `docker-image-tool` that ships with Spark as shown in <<build_exec_containers>>.


.Build the exec containers
[[build_exec_contianers]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/build.sh[tags=build_exec_containers]
----
====



[WARNING]
====
Some parts of Spark may assume a specific layout of the container, e.g. in Spark 3.1 the decommissioning integration makes certain assumptions, so be careful when making changes.
====

== Setting up Kubernetes Permissions


The driver program needs the ability to launch new pods for executors. To allow launching, create a service account or give permissions to the default service account. In my case, I decided to add permissions to the "dask" service account since the JupyterHub launcher (covered later) doesn't support different service accounts depending on the notebook. I also created a special "spark" namespace to make it easier to watch what was happening.

*COMMAND goes here*

== Creating a Service (Allowing Driver-Executor Communication)


Spark depends on the executors connecting back to the driver for both the driver its self and the driver's BlockManager. If your driver is in a different namespace, the easiest way to allow communication is to create a service to let the executors connect to the driver.

*CONFIG goes here*

These port numbers are arbitrary (you can pick different ones), but you'll need to remember them when configuring your SparkContext.

== Configuring Your JupyterHub Launcher


Now that you have all of the foundational components set up, it's time to add them to your JupyterHub launcher. I did this by adding the following to my `config.yaml`:

*CONFIG goes here*

You can then upgrade your previous deployment with the following helm command:

*COMMAND goes here*

== Configuring Your SpakContext


Now that you can launch a notebook with everything needed for Spark, it's time to talk about configuring your SparkContext to work in this environment. You'll need more configuration than you can get through the SparkContext constructor directly, so you will also need to import the SparkConf. Your imports might look like:

*IMPORT goes here*

In my cluster, the K8s API is available at [blah], so I start my configuration with:

*CONFIG goes here*


== Conclusion


The process of adding a Spark notebook to your JupyterHub launcher is a little more involved than it is for typical notebooks because of the required permissions and network connections. Moving inside the cluster from outside of the cluster can offer many advantages, especially if your connection to the cluster goes over the internet. If you aren't familiar with Spark, there is a new version of _Learning Spark_ by my former co-workers (or you can buy the old one I co-wrote, but it's pretty out of date), along with Rachel & my _High Performance Spark_. Up next, I'm planning on deploying Ray on the cluster, then jumping back to Dask and with the GitHub and BitCoin data.
