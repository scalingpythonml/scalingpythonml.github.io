= Running Spark Jupyter Notebooks Client Mode inside of a Kubernetes Cluster (with ARM for Extra Fun)


Having your Spark Notebook inside the same cluster as the executors can reduce network errors and improve uptime. Since these network issues can result in job failure, this is an important consideration. This post assumes that you've already set up the foundation JupyterHub inside of Kubernetes deployment; link:$$https://scalingpythonml.com/2020/12/12/deploying-jupyter-lab-notebook-for-dask-on-arm-on-k8s.html$$[the Dask-distributed notebook blog post covers that if you haven't].


I like to think of this as washing my dog (Timbit) is a lot easier inside of the bath-tun than trying to wash him outside. Although it can take a bit of work to get him inside the tub <<timbit-tub-img>>.

[[timbit-tub-img]]
image::../images/timbit-in-the-tub-IMG_0589.jpg[Timbit in the bath tub]


If you're interested my link:$$https://www.youtube.com/watch?v=a7hDZxisuAk&list=PLRLebp9QyZtapJnz4cpDctnQ1i_qUmeap&index=1$$[YouTube playlist of Get Spark Working with Notebook inside my Kubernetes (K8s/K3s) ARM cluster ] shows the journey I went on to get this working.
A lot of my blog posts come out of my link:$$https://www.youtube.com/user/holdenkarau$$[Open Source Live Streams] (which even include Timbit sometimes).


To get a Spark notebook working inside of the cluster, we need to set up a few different things. The first step, similar to dask-kubernetes, is building a container with Jupyter and Spark installed. We also need to make a container of Spark for the executors. In addition to the containers, we need to set up permissions on the cluster and ensure that the executors that your Spark driver will launch have a way to talk to the driver in the notebook.


[NOTE]
====
It may seem like there are extra steps here compared to dask-kubernetes. Dask-kubernetes automates some service creation, which allows for communication between the scheduler, executors, and the notebook.
====

== Building the Containers


We need two containers, one with Jupyter and Spark installed together and another with just Spark. Since we're working in Python, there are some extra Python libraries we want to install as well (PyArrow, pandas, etc.) If you've got a specific version of a library that your project depends on, you'll want to add it to both the Jupyter Spark driver container and the executor containers.


To start with we'll download link:$$http://spark.apache.org/$$[Apache Spark] and decompress it, as shown in <<dlspark>>, so that we can copy the desired parts inside our containers.

.Download Spark
[[dlspark]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/build.sh[tags=dlspark]
----
====

Now that we have Spark downloaded we can start customizing our `Dockerfiles`.

=== Building the Jupyter Spark Container


The easiest way to build a Jupyter Spark container is to install Spark on top of the base Jupyter container. If you're running on ARM, you'll need to first cross-build the base Jupyter container (see my link:$$https://scalingpythonml.com/2020/12/12/deploying-jupyter-lab-notebook-for-dask-on-arm-on-k8s.html$$[instructions in the previous post]).


In my case I've custom built the link:$$https://github.com/jupyterhub/zero-to-jupyterhub-k8s/tree/master/images/singleuser-sample$$[single-user sample Docker container] from zero-to-jupyterhub-k8s to `holdenk/jupyter-hub-magicsingleuser-sample:0.10.2` as I needed ARM support. If you don't need to cross-build your custom container, you can use the pre-built container at `jupyterhub/k8s-singleuser-sample` as the basis for yours.


Since Spark needs Java to run, I decided to look at the link:$$https://github.com/docker-library/openjdk/blob/master/11/jdk/slim-buster/Dockerfile$$[jdk11 slim dockerfile] to see how to install Java in a dockerfile well. If you're an object-oriented person, you might be wishing we had multiple-inheritence with Dockerfiles, but that doesn't work. In addition to the JDK11 dockerfile, I looked at Spark's own Dockerfiles (includign PySpark) and the resulting Juptyer Spark Container specification is shown in <<spark_notebook_dockerfile>>.

.Dockerfile to add Spark on top of the Jupyter Notebook container.
[[spark_notebook_dockerfile]]
====
[source, dockerfile]
----
include::../subrepos/scalingpythonml/spark/containers/notebook/Dockerfile[]
----
====

Since the Dockerfile copies parts of Spark in, remember to save it at the root of where you decompressed the Spark tarball.


If you're not cross building, you can build this with a regular `docker build`, in my case since I'm targetting arm and x86 I did built it as shown in <<build_spark_nb>>.

.Build Spark notebook container
[[build_spark_nb]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/build.sh[tags=build-notebook]
----
====


[NOTE]
====
An alternative would have been to take the JDK-11 containers as a starting point, and install Jupyter on top of it, but when I tried that I found it more complicated.
====

This gives us a container with both Spark and the base notebook layer together. For the executors, we don't want to bother shipping Jupyter, so we'll build a seperate container for the executors.

=== Building the Executor Container


Spark does not ship pre-built containers for its executors, so regardless of which arch youâ€™re using, you will need to build the executor containers.


If you're building multi-arch containers, you will need to update Spark's docker image tool. You will need to change the buildx option to push the images by adding "--push" to the docker buildx commands in the script for ./bin/docker-image-tool.sh.



Spark's Python container Dockerfile installs an older version of Python without any dependencies, so you will want to customize your Python container setup, as well. My Dockerfile is shown in <<spark_exec_dockerfile>>.



.Dockerfile customizing PySpark setup
[[spark_exec_dockerfile]]
====
[source, dockerfile]
----
include::../subrepos/scalingpythonml/spark/containers/python-executor/Dockerfile[]
----
====

You'll see this file references `pysetup.sh` which installs Python using Miniforge so we can support arm as shown in <<pysetupsh>>.

.Setup python
[[pysetupsh]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/pysetup.sh[]
----
====

You will want to make your Dockerfile install the dependencies for your program while making sure to select the same version of Python that you have in your Jupyter container, so you may need to modify those two examples.


Once you've configured your enviroment, you can build your Spark image using the `docker-image-tool` that ships with Spark as shown in <<build_exec_containers>>.


.Build the exec containers
[[build_exec_contianers]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/containers/build.sh[tags=build_exec_containers]
----
====



[WARNING]
====
Some parts of Spark may assume a specific layout of the container, e.g. in Spark 3.1 the decommissioning integration makes certain assumptions, so be careful when making changes.
====

== Setting up Kubernetes Permissions


The driver program needs the ability to launch new pods for executors. To allow launching, create a service account or give permissions to the default service account (SA) . In my case, I decided to add permissions to the "dask" service account since the JupyterHub launcher (covered later) doesn't support different service accounts depending on the notebook. I also created a special "spark" namespace to make it easier to watch what was happening. My namespace and SA setup is shown in <<setupsa>>.


.Setup up namespace and service account
[[setupsa]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/setup_notebook.sh[tags=setupsa]
----
====


== Creating a Service (Allowing Driver-Executor Communication)


Spark depends on the executors connecting back to the driver for both the driver its self and the driver's BlockManager. If your driver is in a different namespace, the easiest way to allow communication is to create a service to let the executors connect to the driver.

.The Spark Driver Service
[[drvier_svc]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/driver-service.yaml[]
----
====


.Apply the Spark Driver Service
[[drvier_svc_apply]]
====
[source, bash]
----
include::../subrepos/scalingpythonml/spark/setup_notebook.sh[tags=setup_service]
----
====


These port numbers are arbitrary (you can pick different ones), but you'll need to remember them when configuring your SparkContext.

== Configuring Your JupyterHub Launcher


Now that you have all of the foundational components set up, it's time to add them to your JupyterHub launcher. I did this by adding the `Spark 3.0.1` option to the `profileList` in my `config.yaml` shown in <<my-jupyter-config>>.

.Combined Spark and Dask Jupyter Config
[[spark-jupyter-config]]
====
[source, yaml]
----
include::../subrepos/zero-to-jupyterhub-k8s/my-config.yaml[]
----
====


You can then upgrade your previous deployment with `helm upgrade --cleanup-on-fail   --install $RELEASE jupyterhub/jupyterhub   --namespace $NAMESPACE   --create-namespace   --version=0.10.2   --values config.yaml`.

== Configuring Your SpakContext


Now that you can launch a notebook with everything needed for Spark, it's time to talk about configuring your SparkContext to work in this environment. You'll need more configuration than you can get through the SparkContext constructor directly, so you will also need to import the SparkConf. Your imports might look like <<sparkImports>>.

.Spark Imports
[[sparkImports]]
====
[source, python]
----
include::../subrepos/scalingpythonml/spark/PySparkHelloWorldInsideTheCluster.py[tags=sparkImports]
----
====



In my cluster, the K8s API is available at `https://kubernetes.default`, so I start my configuration as in <<makeSparkConf>>.

.Start of Spark Conf
[[makeSparkConf]]
====
[source, python]
----
include::../subrepos/scalingpythonml/spark/PySparkHelloWorldInsideTheCluster.py[tags=makeSparkConf]
----
====

Since there are no pre-built docker images for Spark, you'll need to configure the container image used for the executor, mine is shown in <<configContainer>>.

.Configure Container
[[configContainer]]
====
[source, python]
----
include::../subrepos/scalingpythonml/spark/PySparkHelloWorldInsideTheCluster.py[tags=configureContainer]
----
====



Normally Spark assigns ports randomly for things like the driver and the block manager, but we need to configure Spark to bind to the correct ports, and also have the executors connect to the service we've created instead of trying to connect back to the hostname of the driver. My service configuration is shown in <<sparkNetConf>>.

.Spark Network Conf
[[sparkNetConf]]
====
[source, python]
----
include::../subrepos/scalingpythonml/spark/PySparkHelloWorldInsideTheCluster.py[tags=configureService]
----
====

In addition to that, you'll need to tell Spark which namespace it has permission to create executors in, shown in <<sparkNSConf>>.


.Spark Namespace Conf
[[sparkNSConf]]
====
[source, python]
----
include::../subrepos/scalingpythonml/spark/PySparkHelloWorldInsideTheCluster.py[tags=configureNamespace]
----
====


While it's not essential, configuring an application name makes debugging much easier. You can do this with `.set("spark.app.name", "PySparkHelloWorldInsideTheCluster")`.



== Conclusion


The process of adding a Spark notebook to your JupyterHub launcher is a little more involved than it is for typical notebooks because of the required permissions and network connections. Moving inside the cluster from outside of the cluster can offer many advantages, especially if your connection to the cluster goes over the internet. If you aren't familiar with Spark, there is a new version of link:$$https://amzn.to/2WxB1I1$$[_Learning Spark_] by my former co-workers (or you can buy the link:$$https://amzn.to/2Ww3s98$$[old one I co-wrote], but it's pretty out of date), along with Rachel & my link:$$https://amzn.to/3paoE0L$$[_High Performance Spark_]. Up next, I'm planning on deploying Ray on the cluster, then jumping back to Dask and with the GitHub and BitCoin data.
