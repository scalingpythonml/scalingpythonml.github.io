{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask_kubernetes import KubeCluster\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x7f40b67287c0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify a remote deployment using a load blanacer\n",
    "dask.config.set({\"kubernetes.scheduler-service-type\": \"LoadBalancer\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating scheduler pod on cluster. This may take some time.\n"
     ]
    }
   ],
   "source": [
    "cluster = KubeCluster.from_yaml('worker-spec.yaml', namespace='dask', deploy_mode='remote')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.deploy.adaptive - INFO - Adaptive scaling started: minimum=1 maximum=10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<distributed.deploy.adaptive.Adaptive at 0x7f40b5e6a250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster.adapt(minimum=1, maximum=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://192.168.3.119:8786</li>\n",
       "  <li><b>Dashboard: </b><a href='http://192.168.3.119:8787/status' target='_blank'>http://192.168.3.119:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.42.4.114:8786' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "# Connect Dask to the cluster\n",
    "client = Client(cluster)\n",
    "client # the repr gives us useful links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compression': 'lz4', 'python': (3, 8, 0), 'pickle-protocol': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.scheduler_comm.comm.handshake_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Create a large array and calculate the mean\n",
    "array = da.ones((1000, 1000, 1000))\n",
    "print(array.mean().compute())  # Should print 1.0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know the cluster is doing ok :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure dask to talk to our local MinIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "minio_storage_options = {\n",
    "#    \"anon\": \"false\",\n",
    "    \"key\": \"YOURACCESSKEY\",\n",
    "    \"secret\": \"YOURSECRETKEY\",\n",
    "    \"client_kwargs\": {\n",
    "        \"endpoint_url\": \"http://minio-1602984784.minio.svc.cluster.local:9000\",\n",
    "        \"region_name\": 'us-east-1'\n",
    "    },\n",
    "    \"config_kwargs\": {\"s3\": {\"signature_version\": 's3v4'}},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the GH archive data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date=datetime.datetime(2020,10,1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gh_archive_files=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "while current_date < datetime.datetime.now() -  datetime.timedelta(days=1):\n",
    "    current_date = current_date + datetime.timedelta(hours=1)\n",
    "    datestring = f'{current_date.year}-{current_date.month:02}-{current_date.day:02}-{current_date.hour}'\n",
    "    gh_url = f'http://data.githubarchive.org/{datestring}.json.gz'\n",
    "    gh_archive_files.append(gh_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.deploy.adaptive - INFO - Retiring workers [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "df = dd.read_json(gh_archive_files[0], compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92084"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': {'class': 'fsspec.implementations.local.LocalFileSystem'},\n",
       " 'memory': {'class': 'fsspec.implementations.memory.MemoryFileSystem'},\n",
       " 'dropbox': {'class': 'dropboxdrivefs.DropboxDriveFileSystem',\n",
       "  'err': 'DropboxFileSystem requires \"dropboxdrivefs\",\"requests\" and \"dropbox\" to be installed'},\n",
       " 'http': {'class': 'fsspec.implementations.http.HTTPFileSystem',\n",
       "  'err': 'HTTPFileSystem requires \"requests\" and \"aiohttp\" to be installed'},\n",
       " 'https': {'class': 'fsspec.implementations.http.HTTPFileSystem',\n",
       "  'err': 'HTTPFileSystem requires \"requests\" and \"aiohttp\" to be installed'},\n",
       " 'zip': {'class': 'fsspec.implementations.zip.ZipFileSystem'},\n",
       " 'gcs': {'class': 'gcsfs.GCSFileSystem',\n",
       "  'err': 'Please install gcsfs to access Google Storage'},\n",
       " 'gs': {'class': 'gcsfs.GCSFileSystem',\n",
       "  'err': 'Please install gcsfs to access Google Storage'},\n",
       " 'gdrive': {'class': 'gdrivefs.GoogleDriveFileSystem',\n",
       "  'err': 'Please install gdrivefs for access to Google Drive'},\n",
       " 'sftp': {'class': 'fsspec.implementations.sftp.SFTPFileSystem',\n",
       "  'err': 'SFTPFileSystem requires \"paramiko\" to be installed'},\n",
       " 'ssh': {'class': 'fsspec.implementations.sftp.SFTPFileSystem',\n",
       "  'err': 'SFTPFileSystem requires \"paramiko\" to be installed'},\n",
       " 'ftp': {'class': 'fsspec.implementations.ftp.FTPFileSystem'},\n",
       " 'hdfs': {'class': 'fsspec.implementations.hdfs.PyArrowHDFS',\n",
       "  'err': 'pyarrow and local java libraries required for HDFS'},\n",
       " 'webhdfs': {'class': 'fsspec.implementations.webhdfs.WebHDFS',\n",
       "  'err': 'webHDFS access requires \"requests\" to be installed'},\n",
       " 's3': {'class': 's3fs.S3FileSystem', 'err': 'Install s3fs to access S3'},\n",
       " 'adl': {'class': 'adlfs.AzureDatalakeFileSystem',\n",
       "  'err': 'Install adlfs to access Azure Datalake Gen1'},\n",
       " 'abfs': {'class': 'adlfs.AzureBlobFileSystem',\n",
       "  'err': 'Install adlfs to access Azure Datalake Gen2 and Azure Blob Storage'},\n",
       " 'az': {'class': 'adlfs.AzureBlobFileSystem',\n",
       "  'err': 'Install adlfs to access Azure Datalake Gen2 and Azure Blob Storage'},\n",
       " 'cached': {'class': 'fsspec.implementations.cached.CachingFileSystem'},\n",
       " 'blockcache': {'class': 'fsspec.implementations.cached.CachingFileSystem'},\n",
       " 'filecache': {'class': 'fsspec.implementations.cached.WholeFileCacheFileSystem'},\n",
       " 'simplecache': {'class': 'fsspec.implementations.cached.SimpleCacheFileSystem'},\n",
       " 'dask': {'class': 'fsspec.implementations.dask.DaskWorkerFileSystem',\n",
       "  'err': 'Install dask distributed to access worker file system'},\n",
       " 'github': {'class': 'fsspec.implementations.github.GithubFileSystem',\n",
       "  'err': 'Install the requests package to use the github FS'},\n",
       " 'git': {'class': 'fsspec.implementations.git.GitFileSystem',\n",
       "  'err': 'Install pygit2 to browse local git repos'},\n",
       " 'smb': {'class': 'fsspec.implementations.smb.SMBFileSystem',\n",
       "  'err': 'SMB requires \"smbprotocol\" or \"smbprotocol[kerberos]\" installed'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fsspec.registry import known_implementations\n",
    "known_implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'type', 'actor', 'repo', 'payload', 'public', 'created_at',\n",
       "       'org'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dask-test/boop-test/0.part']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_csv(\"s3://dask-test/boop-test\", storage_options=minio_storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.value().result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.value().result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter.value().result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a large array and calculate the mean\n",
    "array = da.ones((1000, 1000, 1000))\n",
    "print(array.mean().compute())  # Should print 1.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.client import Config\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3',\n",
    "                    endpoint_url='http://localhost:9000',\n",
    "                    aws_access_key_id='YOURACCESSKEY',\n",
    "                    aws_secret_access_key='YOURSECRETKEY',\n",
    "                    config=Config(signature_version='s3v4'),\n",
    "                    region_name='us-east-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.Bucket('dask-test').upload_file('/etc/lsb-release','lsb-release')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
