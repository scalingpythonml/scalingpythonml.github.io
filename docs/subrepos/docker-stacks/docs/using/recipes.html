<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Contributed Recipes | Scaling Python ML</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Contributed Recipes" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Blog of my adventures working with different tools for scaling Python ML workloads." />
<meta property="og:description" content="Blog of my adventures working with different tools for scaling Python ML workloads." />
<meta property="og:site_name" content="Scaling Python ML" />
<script type="application/ld+json">
{"url":"/subrepos/docker-stacks/docs/using/recipes.html","description":"Blog of my adventures working with different tools for scaling Python ML workloads.","headline":"Contributed Recipes","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Scaling Python ML" />

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Scaling Python ML</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a><a class="page-link" href="/mailinglist.html">Mailinglist</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Contributed Recipes</h1>
  </header>

  <div class="post-content">
    <p>Users sometimes share interesting ways of using the Jupyter Docker Stacks. We encourage users to
<a href="/subrepos/docker-stacks/docs/contributing/recipes.html">contribute these recipes</a> to the documentation in case they prove
useful to other members of the community by submitting a pull request to <code class="language-plaintext highlighter-rouge">docs/using/recipes.md</code>.
The sections below capture this knowledge.</p>

<h2 id="using-sudo-within-a-container">Using <code class="language-plaintext highlighter-rouge">sudo</code> within a container</h2>

<p>Password authentication is disabled for the <code class="language-plaintext highlighter-rouge">NB_USER</code> (e.g., <code class="language-plaintext highlighter-rouge">jovyan</code>). This choice was made to
avoid distributing images with a weak default password that users ~might~ will forget to change
before running a container on a publicly accessible host.</p>

<p>You can grant the within-container <code class="language-plaintext highlighter-rouge">NB_USER</code> passwordless <code class="language-plaintext highlighter-rouge">sudo</code> access by adding
<code class="language-plaintext highlighter-rouge">-e GRANT_SUDO=yes</code> and <code class="language-plaintext highlighter-rouge">--user root</code> to your Docker command line or appropriate container
orchestrator config.</p>

<p>For example:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-it</span> <span class="nt">-e</span> <span class="nv">GRANT_SUDO</span><span class="o">=</span><span class="nb">yes</span> <span class="nt">--user</span> root jupyter/minimal-notebook
</code></pre></div></div>

<p><strong>You should only enable <code class="language-plaintext highlighter-rouge">sudo</code> if you trust the user and/or if the container is running on an
isolated host.</strong> See <a href="https://docs.docker.com/engine/security/userns-remap/">Docker security documentation</a> for more information about running containers as <code class="language-plaintext highlighter-rouge">root</code>.</p>

<h2 id="using-pip-install-or-conda-install-in-a-child-docker-image">Using <code class="language-plaintext highlighter-rouge">pip install</code> or <code class="language-plaintext highlighter-rouge">conda install</code> in a Child Docker image</h2>

<p>Create a new Dockerfile like the one shown below.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start from a core stack version</span>
<span class="k">FROM</span><span class="s"> jupyter/datascience-notebook:9f9e5ca8fe5a</span>
<span class="c"># Install in the default python3 environment</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="s1">'ggplot==0.6.8'</span>
</code></pre></div></div>

<p>Then build a new image.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">--rm</span> <span class="nt">-t</span> jupyter/my-datascience-notebook <span class="nb">.</span>
</code></pre></div></div>

<p>To use a requirements.txt file, first create your <code class="language-plaintext highlighter-rouge">requirements.txt</code> file with the listing of
packages desired. Next, create a new Dockerfile like the one shown below.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start from a core stack version</span>
<span class="k">FROM</span><span class="s"> jupyter/datascience-notebook:9f9e5ca8fe5a</span>
<span class="c"># Install from requirements.txt file</span>
<span class="k">COPY</span><span class="s"> --chown=${NB_UID}:${NB_GID} requirements.txt /tmp/</span>
<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--requirement</span> /tmp/requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>    fix-permissions <span class="nv">$CONDA_DIR</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    fix-permissions /home/<span class="nv">$NB_USER</span>
</code></pre></div></div>

<p>For conda, the Dockerfile is similar:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start from a core stack version</span>
<span class="k">FROM</span><span class="s"> jupyter/datascience-notebook:9f9e5ca8fe5a</span>
<span class="c"># Install from requirements.txt file</span>
<span class="k">COPY</span><span class="s"> --chown=${NB_UID}:${NB_GID} requirements.txt /tmp/</span>
<span class="k">RUN </span>conda <span class="nb">install</span> <span class="nt">--yes</span> <span class="nt">--file</span> /tmp/requirements.txt <span class="o">&amp;&amp;</span> <span class="se">\
</span>    fix-permissions <span class="nv">$CONDA_DIR</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    fix-permissions /home/<span class="nv">$NB_USER</span>
</code></pre></div></div>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/commit/79169618d571506304934a7b29039085e77db78c#commitcomment-15960081">docker-stacks/commit/79169618d571506304934a7b29039085e77db78c</a></p>

<h2 id="add-a-python-2x-environment">Add a Python 2.x environment</h2>

<p>Python 2.x was removed from all images on August 10th, 2017, starting in tag <code class="language-plaintext highlighter-rouge">cc9feab481f7</code>. You can
add a Python 2.x environment by defining your own Dockerfile inheriting from one of the images like
so:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Choose your desired base image</span>
<span class="k">FROM</span><span class="s"> jupyter/scipy-notebook:latest</span>

<span class="c"># Create a Python 2.x environment using conda including at least the ipython kernel</span>
<span class="c"># and the kernda utility. Add any additional packages you want available for use</span>
<span class="c"># in a Python 2 notebook to the first line here (e.g., pandas, matplotlib, etc.)</span>
<span class="k">RUN </span>conda create <span class="nt">--quiet</span> <span class="nt">--yes</span> <span class="nt">-p</span> <span class="nv">$CONDA_DIR</span>/envs/python2 <span class="nv">python</span><span class="o">=</span>2.7 ipython ipykernel kernda <span class="o">&amp;&amp;</span> <span class="se">\
</span>    conda clean <span class="nt">--all</span> <span class="nt">-f</span> <span class="nt">-y</span>

<span class="k">USER</span><span class="s"> root</span>

<span class="c"># Create a global kernelspec in the image and modify it so that it properly activates</span>
<span class="c"># the python2 conda environment.</span>
<span class="k">RUN </span><span class="nv">$CONDA_DIR</span>/envs/python2/bin/python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span><span class="nv">$CONDA_DIR</span>/envs/python2/bin/kernda <span class="nt">-o</span> <span class="nt">-y</span> /usr/local/share/jupyter/kernels/python2/kernel.json

<span class="k">USER</span><span class="s"> $NB_USER</span>
</code></pre></div></div>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/440">https://github.com/jupyter/docker-stacks/issues/440</a></p>

<h2 id="add-a-python-3x-environment">Add a Python 3.x environment</h2>

<p>The default version of Python that ships with conda/ubuntu may not be the version you want.
To add a conda environment with a different version and make it accessible to Jupyter, the instructions are very similar to Python 2.x but are slightly simpler (no need to switch to <code class="language-plaintext highlighter-rouge">root</code>):</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Choose your desired base image</span>
<span class="k">FROM</span><span class="s"> jupyter/minimal-notebook:latest</span>

<span class="c"># name your environment and choose python 3.x version</span>
<span class="k">ARG</span><span class="s"> conda_env=python36</span>
<span class="k">ARG</span><span class="s"> py_ver=3.6</span>

<span class="c"># you can add additional libraries you want conda to install by listing them below the first line and ending with "&amp;&amp; \"</span>
<span class="k">RUN </span>conda create <span class="nt">--quiet</span> <span class="nt">--yes</span> <span class="nt">-p</span> <span class="nv">$CONDA_DIR</span>/envs/<span class="nv">$conda_env</span> <span class="nv">python</span><span class="o">=</span><span class="nv">$py_ver</span> ipython ipykernel <span class="o">&amp;&amp;</span> <span class="se">\
</span>    conda clean <span class="nt">--all</span> <span class="nt">-f</span> <span class="nt">-y</span>

<span class="c"># alternatively, you can comment out the lines above and uncomment those below</span>
<span class="c"># if you'd prefer to use a YAML file present in the docker build context</span>

<span class="c"># COPY --chown=${NB_UID}:${NB_GID} environment.yml /home/$NB_USER/tmp/</span>
<span class="c"># RUN cd /home/$NB_USER/tmp/ &amp;&amp; \</span>
<span class="c">#     conda env create -p $CONDA_DIR/envs/$conda_env -f environment.yml &amp;&amp; \</span>
<span class="c">#     conda clean --all -f -y</span>


<span class="c"># create Python 3.x environment and link it to jupyter</span>
<span class="k">RUN </span><span class="nv">$CONDA_DIR</span>/envs/<span class="k">${</span><span class="nv">conda_env</span><span class="k">}</span>/bin/python <span class="nt">-m</span> ipykernel <span class="nb">install</span> <span class="nt">--user</span> <span class="nt">--name</span><span class="o">=</span><span class="k">${</span><span class="nv">conda_env</span><span class="k">}</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    fix-permissions <span class="nv">$CONDA_DIR</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    fix-permissions /home/<span class="nv">$NB_USER</span>

<span class="c"># any additional pip installs can be added by uncommenting the following line</span>
<span class="c"># RUN $CONDA_DIR/envs/${conda_env}/bin/pip install </span>

<span class="c"># prepend conda environment to path</span>
<span class="k">ENV</span><span class="s"> PATH $CONDA_DIR/envs/${conda_env}/bin:$PATH</span>

<span class="c"># if you want this environment to be the default one, uncomment the following line:</span>
<span class="c"># ENV CONDA_DEFAULT_ENV ${conda_env}</span>
</code></pre></div></div>

<h2 id="run-jupyterlab">Run JupyterLab</h2>

<p>JupyterLab is preinstalled as a notebook extension starting in tag
<a href="https://github.com/jupyter/docker-stacks/wiki/Docker-build-history">c33a7dc0eece</a>.</p>

<p>Run jupyterlab using a command such as
<code class="language-plaintext highlighter-rouge">docker run -it --rm -p 8888:8888 jupyter/datascience-notebook start.sh jupyter lab</code></p>

<h2 id="dask-jupyterlab-extension">Dask JupyterLab Extension</h2>

<p><a href="https://github.com/dask/dask-labextension">Dask JupyterLab Extension</a> provides a JupyterLab extension to manage Dask clusters, as well as embed Dask’s dashboard plots directly into JupyterLab panes. Create the Dockerfile as:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Start from a core stack version</span>
<span class="k">FROM</span><span class="s"> jupyter/scipy-notebook:latest</span>

<span class="c"># Install the Dask dashboard</span>
<span class="k">RUN </span>pip <span class="nb">install </span>dask_labextension <span class="p">;</span> <span class="se">\
</span>    jupyter labextension <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">--clean</span> <span class="se">\
</span>    dask-labextension

<span class="c"># Dask Scheduler &amp; Bokeh ports</span>
<span class="k">EXPOSE</span><span class="s"> 8787</span>
<span class="k">EXPOSE</span><span class="s"> 8786</span>

<span class="k">ENTRYPOINT</span><span class="s"> ["jupyter", "lab", "--ip=0.0.0.0", "--allow-root"]</span>
</code></pre></div></div>

<p>And build the image as:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker build <span class="nt">-t</span> jupyter/scipy-dasklabextension:latest <span class="nb">.</span>
</code></pre></div></div>

<p>Once built, run using the command:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">-p</span> 8888:8888 <span class="nt">-p</span> 8787:8787 jupyter/scipy-dasklabextension:latest
</code></pre></div></div>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/999">https://github.com/jupyter/docker-stacks/issues/999</a></p>

<h2 id="lets-encrypt-a-notebook-server">Let’s Encrypt a Notebook server</h2>

<p>See the README for the simple automation here
<a href="https://github.com/jupyter/docker-stacks/tree/master/examples/make-deploy">https://github.com/jupyter/docker-stacks/tree/master/examples/make-deploy</a>
which includes steps for requesting and renewing a Let’s Encrypt certificate.</p>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/78">https://github.com/jupyter/docker-stacks/issues/78</a></p>

<h2 id="slideshows-with-jupyter-and-rise">Slideshows with Jupyter and RISE</h2>

<p><a href="https://github.com/damianavila/RISE">RISE</a> allows via extension to create live slideshows of your
notebooks, with no conversion, adding javascript Reveal.js:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Add Live slideshows with RISE</span>
RUN conda <span class="nb">install</span> <span class="nt">-c</span> damianavila82 rise
</code></pre></div></div>

<p>Credit: <a href="https://github.com/pdonorio">Paolo D.</a> based on
<a href="https://github.com/jupyter/docker-stacks/issues/43">docker-stacks/issues/43</a></p>

<h2 id="xgboost">xgboost</h2>

<p>You need to install conda’s gcc for Python xgboost to work properly. Otherwise, you’ll get an
exception about libgomp.so.1 missing GOMP_4.0.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%%bash
conda <span class="nb">install</span> <span class="nt">-y</span> gcc
pip <span class="nb">install </span>xgboost

import xgboost
</code></pre></div></div>

<h2 id="running-behind-a-nginx-proxy">Running behind a nginx proxy</h2>

<p>Sometimes it is useful to run the Jupyter instance behind a nginx proxy, for instance:</p>

<ul>
  <li>you would prefer to access the notebook at a server URL with a path
(<code class="language-plaintext highlighter-rouge">https://example.com/jupyter</code>) rather than a port (<code class="language-plaintext highlighter-rouge">https://example.com:8888</code>)</li>
  <li>you may have many different services in addition to Jupyter running on the same server, and want
to nginx to help improve server performance in manage the connections</li>
</ul>

<p>Here is a <a href="https://gist.github.com/cboettig/8643341bd3c93b62b5c2">quick example NGINX configuration</a>
to get started. You’ll need a server, a <code class="language-plaintext highlighter-rouge">.crt</code> and <code class="language-plaintext highlighter-rouge">.key</code> file for your server, and <code class="language-plaintext highlighter-rouge">docker</code> &amp;
<code class="language-plaintext highlighter-rouge">docker-compose</code> installed. Then just download the files at that gist and run <code class="language-plaintext highlighter-rouge">docker-compose up -d</code>
to test it out. Customize the <code class="language-plaintext highlighter-rouge">nginx.conf</code> file to set the desired paths and add other services.</p>

<h2 id="host-volume-mounts-and-notebook-errors">Host volume mounts and notebook errors</h2>

<p>If you are mounting a host directory as <code class="language-plaintext highlighter-rouge">/home/jovyan/work</code> in your container and you receive
permission errors or connection errors when you create a notebook, be sure that the <code class="language-plaintext highlighter-rouge">jovyan</code> user
(UID=1000 by default) has read/write access to the directory on the host. Alternatively, specify the
UID of the <code class="language-plaintext highlighter-rouge">jovyan</code> user on container startup using the <code class="language-plaintext highlighter-rouge">-e NB_UID</code> option described in the
<a href="../using/common.html#Docker-Options">Common Features, Docker Options section</a></p>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/199">https://github.com/jupyter/docker-stacks/issues/199</a></p>

<h2 id="manpage-installation">Manpage installation</h2>

<p>Most containers, including our Ubuntu base image, ship without manpages installed to save space. You
can use the following dockerfile to inherit from one of our images to enable manpages:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Choose your desired base image</span>
<span class="k">ARG</span><span class="s"> BASE_CONTAINER=jupyter/datascience-notebook:latest</span>
<span class="k">FROM</span><span class="s"> $BASE_CONTAINER</span>

<span class="k">USER</span><span class="s"> root</span>

<span class="c"># Remove the manpage blacklist, install man, install docs</span>
<span class="k">RUN </span><span class="nb">rm</span> /etc/dpkg/dpkg.cfg.d/excludes <span class="se">\
</span>    <span class="o">&amp;&amp;</span> apt-get update <span class="se">\
</span>    <span class="o">&amp;&amp;</span> dpkg <span class="nt">-l</span> | <span class="nb">grep</span> ^ii | <span class="nb">cut</span> <span class="nt">-d</span><span class="s1">' '</span> <span class="nt">-f3</span> | xargs apt-get <span class="nb">install</span> <span class="nt">-yq</span> <span class="nt">--no-install-recommends</span> <span class="nt">--reinstall</span> man <span class="se">\
</span>    <span class="o">&amp;&amp;</span> apt-get clean <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>

<span class="k">USER</span><span class="s"> $NB_UID</span>
</code></pre></div></div>

<p>Adding the documentation on top of an existing singleuser image wastes a lot of space and requires
reinstalling every system package, which can take additional time and bandwidth; the
<code class="language-plaintext highlighter-rouge">datascience-notebook</code> image has been shown to grow by almost 3GB when adding manapages in this way.
Enabling manpages in the base Ubuntu layer prevents this container bloat.
Just use previous <code class="language-plaintext highlighter-rouge">Dockerfile</code> with original ubuntu image as your base container:</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ubuntu 20.04 (focal) from 2020-04-23</span>
<span class="c"># https://github.com/docker-library/official-images/commit/4475094895093bcc29055409494cce1e11b52f94</span>
<span class="k">ARG</span><span class="s"> BASE_CONTAINER=ubuntu:focal-20200423@sha256:238e696992ba9913d24cfc3727034985abd136e08ee3067982401acdc30cbf3f</span>
</code></pre></div></div>

<p>For Ubuntu 18.04 (bionic) and earlier, you may also require to workaround for a mandb bug, which was fixed in mandb &gt;= 2.8.6.1:</p>
<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># https://git.savannah.gnu.org/cgit/man-db.git/commit/?id=8197d7824f814c5d4b992b4c8730b5b0f7ec589a</span>
<span class="c"># http://launchpadlibrarian.net/435841763/man-db_2.8.5-2_2.8.6-1.diff.gz</span>

<span class="k">RUN </span><span class="nb">echo</span> <span class="s2">"MANPATH_MAP </span><span class="k">${</span><span class="nv">CONDA_DIR</span><span class="k">}</span><span class="s2">/bin </span><span class="k">${</span><span class="nv">CONDA_DIR</span><span class="k">}</span><span class="s2">/man"</span> <span class="o">&gt;&gt;</span> /etc/manpath.config <span class="se">\
</span>    <span class="o">&amp;&amp;</span> <span class="nb">echo</span> <span class="s2">"MANPATH_MAP </span><span class="k">${</span><span class="nv">CONDA_DIR</span><span class="k">}</span><span class="s2">/bin </span><span class="k">${</span><span class="nv">CONDA_DIR</span><span class="k">}</span><span class="s2">/share/man"</span> <span class="o">&gt;&gt;</span> /etc/manpath.config <span class="se">\
</span>    <span class="o">&amp;&amp;</span> mandb

</code></pre></div></div>

<p>Be sure to check the current base image in <code class="language-plaintext highlighter-rouge">base-notebook</code> before building.</p>

<h2 id="jupyterhub">JupyterHub</h2>

<p>We also have contributed recipes for using JupyterHub.</p>

<h3 id="use-jupyterhubs-dockerspawner">Use JupyterHub’s dockerspawner</h3>

<p>In most cases for use with DockerSpawner, given any image that already has a notebook stack set up,
you would only need to add:</p>

<ol>
  <li>install the jupyterhub-singleuser script (for the right Python)</li>
  <li>change the command to launch the single-user server</li>
</ol>

<p>Swapping out the <code class="language-plaintext highlighter-rouge">FROM</code> line in the <code class="language-plaintext highlighter-rouge">jupyterhub/singleuser</code> Dockerfile should be enough for most
cases.</p>

<p>Credit: <a href="https://github.com/jtyberg">Justin Tyberg</a>, <a href="https://github.com/quanghoc">quanghoc</a>, and
<a href="https://github.com/minrk">Min RK</a> based on
<a href="https://github.com/jupyter/docker-stacks/issues/124">docker-stacks/issues/124</a> and
<a href="https://github.com/jupyter/docker-stacks/pull/185">docker-stacks/pull/185</a></p>

<h3 id="containers-with-a-specific-version-of-jupyterhub">Containers with a specific version of JupyterHub</h3>

<p>To use a specific version of JupyterHub, the version of <code class="language-plaintext highlighter-rouge">jupyterhub</code> in your image should match the
version in the Hub itself.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> jupyter/base-notebook:5ded1de07260</span>
<span class="k">RUN </span>pip <span class="nb">install </span><span class="nv">jupyterhub</span><span class="o">==</span>0.8.0b1
</code></pre></div></div>

<p>Credit: <a href="https://github.com/jupyter/docker-stacks/issues/423#issuecomment-322767742">MinRK</a></p>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/177">https://github.com/jupyter/docker-stacks/issues/177</a></p>

<h2 id="spark">Spark</h2>

<p>A few suggestions have been made regarding using Docker Stacks with spark.</p>

<h3 id="using-pyspark-with-aws-s3">Using PySpark with AWS S3</h3>

<p>Using Spark session for hadoop 2.7.3</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="c1"># !ls /usr/local/spark/jars/hadoop* # to figure out what version of hadoop
</span><span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'PYSPARK_SUBMIT_ARGS'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'--packages "org.apache.hadoop:hadoop-aws:2.7.3" pyspark-shell'</span>

<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="n">myAccessKey</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span>
<span class="n">mySecretKey</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span>

<span class="n">spark</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">sql</span><span class="p">.</span><span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span> \
        <span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.hadoop.fs.s3a.access.key"</span><span class="p">,</span> <span class="n">myAccessKey</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">config</span><span class="p">(</span><span class="s">"spark.hadoop.fs.s3a.secret.key"</span><span class="p">,</span> <span class="n">mySecretKey</span><span class="p">)</span> \
        <span class="p">.</span><span class="n">getOrCreate</span><span class="p">()</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">"s3://myBucket/myKey"</span><span class="p">)</span>
</code></pre></div></div>

<p>Using Spark context for hadoop 2.6.0</p>

<div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'PYSPARK_SUBMIT_ARGS'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'--packages com.amazonaws:aws-java-sdk:1.10.34,org.apache.hadoop:hadoop-aws:2.6.0 pyspark-shell'</span>

<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">SparkContext</span><span class="p">(</span><span class="s">"local[*]"</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span>
<span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>

<span class="n">hadoopConf</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">_jsc</span><span class="p">.</span><span class="n">hadoopConfiguration</span><span class="p">()</span>
<span class="n">myAccessKey</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span>
<span class="n">mySecretKey</span> <span class="o">=</span> <span class="nb">input</span><span class="p">()</span>
<span class="n">hadoopConf</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"fs.s3.impl"</span><span class="p">,</span> <span class="s">"org.apache.hadoop.fs.s3native.NativeS3FileSystem"</span><span class="p">)</span>
<span class="n">hadoopConf</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"fs.s3.awsAccessKeyId"</span><span class="p">,</span> <span class="n">myAccessKey</span><span class="p">)</span>
<span class="n">hadoopConf</span><span class="p">.</span><span class="nb">set</span><span class="p">(</span><span class="s">"fs.s3.awsSecretAccessKey"</span><span class="p">,</span> <span class="n">mySecretKey</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="p">.</span><span class="n">read</span><span class="p">.</span><span class="n">parquet</span><span class="p">(</span><span class="s">"s3://myBucket/myKey"</span><span class="p">)</span>
</code></pre></div></div>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/127">https://github.com/jupyter/docker-stacks/issues/127</a></p>

<h3 id="using-local-spark-jars">Using Local Spark JARs</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">'PYSPARK_SUBMIT_ARGS'</span><span class="p">]</span> <span class="o">=</span> <span class="s">'--jars /home/jovyan/spark-streaming-kafka-assembly_2.10-1.6.1.jar pyspark-shell'</span>
<span class="kn">import</span> <span class="nn">pyspark</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming.kafka</span> <span class="kn">import</span> <span class="n">KafkaUtils</span>
<span class="kn">from</span> <span class="nn">pyspark.streaming</span> <span class="kn">import</span> <span class="n">StreamingContext</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">pyspark</span><span class="p">.</span><span class="n">SparkContext</span><span class="p">()</span>
<span class="n">ssc</span> <span class="o">=</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">broker</span> <span class="o">=</span> <span class="s">"&lt;my_broker_ip&gt;"</span>
<span class="n">directKafkaStream</span> <span class="o">=</span> <span class="n">KafkaUtils</span><span class="p">.</span><span class="n">createDirectStream</span><span class="p">(</span><span class="n">ssc</span><span class="p">,</span> <span class="p">[</span><span class="s">"test1"</span><span class="p">],</span> <span class="p">{</span><span class="s">"metadata.broker.list"</span><span class="p">:</span> <span class="n">broker</span><span class="p">})</span>
<span class="n">directKafkaStream</span><span class="p">.</span><span class="n">pprint</span><span class="p">()</span>
<span class="n">ssc</span><span class="p">.</span><span class="n">start</span><span class="p">()</span>
</code></pre></div></div>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/154">https://github.com/jupyter/docker-stacks/issues/154</a></p>

<h3 id="using-spark-packagesorg">Using spark-packages.org</h3>

<p>If you’d like to use packages from <a href="https://spark-packages.org/">spark-packages.org</a>, see
<a href="https://gist.github.com/parente/c95fdaba5a9a066efaab">https://gist.github.com/parente/c95fdaba5a9a066efaab</a>
for an example of how to specify the package identifier in the environment before creating a
SparkContext.</p>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/43">https://github.com/jupyter/docker-stacks/issues/43</a></p>

<h3 id="use-jupyterall-spark-notebooks-with-an-existing-sparkyarn-cluster">Use jupyter/all-spark-notebooks with an existing Spark/YARN cluster</h3>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">FROM</span><span class="s"> jupyter/all-spark-notebook</span>

<span class="c"># Set env vars for pydoop</span>
<span class="k">ENV</span><span class="s"> HADOOP_HOME /usr/local/hadoop-2.7.3</span>
<span class="k">ENV</span><span class="s"> JAVA_HOME /usr/lib/jvm/java-8-openjdk-amd64</span>
<span class="k">ENV</span><span class="s"> HADOOP_CONF_HOME /usr/local/hadoop-2.7.3/etc/hadoop</span>
<span class="k">ENV</span><span class="s"> HADOOP_CONF_DIR  /usr/local/hadoop-2.7.3/etc/hadoop</span>

<span class="k">USER</span><span class="s"> root</span>
<span class="c"># Add proper open-jdk-8 not just the jre, needed for pydoop</span>
<span class="k">RUN </span><span class="nb">echo</span> <span class="s1">'deb http://cdn-fastly.deb.debian.org/debian jessie-backports main'</span> <span class="o">&gt;</span> /etc/apt/sources.list.d/jessie-backports.list <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nt">-y</span> update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">--no-install-recommends</span> <span class="nt">-t</span> jessie-backports <span class="nt">-y</span> openjdk-8-jdk <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">rm</span> /etc/apt/sources.list.d/jessie-backports.list <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get clean <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/ <span class="o">&amp;&amp;</span> <span class="se">\
</span><span class="c"># Add hadoop binaries</span>
    wget http://mirrors.ukfast.co.uk/sites/ftp.apache.org/hadoop/common/hadoop-2.7.3/hadoop-2.7.3.tar.gz &amp;&amp; \
    tar -xvf hadoop-2.7.3.tar.gz -C /usr/local &amp;&amp; \
    chown -R $NB_USER:users /usr/local/hadoop-2.7.3 &amp;&amp; \
    rm -f hadoop-2.7.3.tar.gz &amp;&amp; \
# Install os dependencies required for pydoop, pyhive
    apt-get update &amp;&amp; \
    apt-get install --no-install-recommends -y build-essential python-dev libsasl2-dev &amp;&amp; \
    apt-get clean &amp;&amp; \
    rm -rf /var/lib/apt/lists/* &amp;&amp; \
# Remove the example hadoop configs and replace
<span class="c"># with those for our cluster.</span>
<span class="c"># Alternatively this could be mounted as a volume</span>
    rm -f /usr/local/hadoop-2.7.3/etc/hadoop/*

# Download this from ambari / cloudera manager and copy here
<span class="k">COPY</span><span class="s"> example-hadoop-conf/ /usr/local/hadoop-2.7.3/etc/hadoop/</span>

<span class="c"># Spark-Submit doesn't work unless I set the following</span>
<span class="k">RUN </span><span class="nb">echo</span> <span class="s2">"spark.driver.extraJavaOptions -Dhdp.version=2.5.3.0-37"</span> <span class="o">&gt;&gt;</span> /usr/local/spark/conf/spark-defaults.conf  <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">echo</span> <span class="s2">"spark.yarn.am.extraJavaOptions -Dhdp.version=2.5.3.0-37"</span> <span class="o">&gt;&gt;</span> /usr/local/spark/conf/spark-defaults.conf <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">echo</span> <span class="s2">"spark.master=yarn"</span> <span class="o">&gt;&gt;</span>  /usr/local/spark/conf/spark-defaults.conf <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">echo</span> <span class="s2">"spark.hadoop.yarn.timeline-service.enabled=false"</span> <span class="o">&gt;&gt;</span> /usr/local/spark/conf/spark-defaults.conf <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">chown</span> <span class="nt">-R</span> <span class="nv">$NB_USER</span>:users /usr/local/spark/conf/spark-defaults.conf <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="c"># Create an alternative HADOOP_CONF_HOME so we can mount as a volume and repoint</span>
    <span class="c"># using ENV var if needed</span>
    mkdir -p /etc/hadoop/conf/ &amp;&amp; \
    chown $NB_USER:users /etc/hadoop/conf/

<span class="k">USER</span><span class="s"> $NB_USER</span>

<span class="c"># Install useful jupyter extensions and python libraries like :</span>
<span class="c"># - Dashboards</span>
<span class="c"># - PyDoop</span>
<span class="c"># - PyHive</span>
<span class="k">RUN </span>pip <span class="nb">install </span>jupyter_dashboards faker <span class="o">&amp;&amp;</span> <span class="se">\
</span>    jupyter dashboards quick-setup <span class="nt">--sys-prefix</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    pip2 <span class="nb">install </span>pyhive pydoop thrift sasl thrift_sasl faker

<span class="k">USER</span><span class="s"> root</span>
<span class="c"># Ensure we overwrite the kernel config so that toree connects to cluster</span>
<span class="k">RUN </span>jupyter toree <span class="nb">install</span> <span class="nt">--sys-prefix</span> <span class="nt">--spark_opts</span><span class="o">=</span><span class="s2">"--master yarn --deploy-mode client --driver-memory 512m  --executor-memory 512m  --executor-cores 1 --driver-java-options -Dhdp.version=2.5.3.0-37 --conf spark.hadoop.yarn.timeline-service.enabled=false"</span>
<span class="k">USER</span><span class="s"> $NB_USER</span>
</code></pre></div></div>

<p>Credit: <a href="https://github.com/britishbadger">britishbadger</a> from
<a href="https://github.com/jupyter/docker-stacks/issues/369">docker-stacks/issues/369</a></p>

<h2 id="run-jupyter-notebooklab-inside-an-already-secured-environment-ie-with-no-token">Run Jupyter Notebook/Lab inside an already secured environment (i.e., with no token)</h2>

<p>(Adapted from <a href="https://github.com/jupyter/docker-stacks/issues/728">issue 728</a>)</p>

<p>The default security is very good. There are use cases, encouraged by containers, where the jupyter
container and the system it runs within, lie inside the security boundary. In these use cases it is
convenient to launch the server without a password or token. In this case, you should use the
<code class="language-plaintext highlighter-rouge">start.sh</code> script to launch the server with no token:</p>

<p>For jupyterlab:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run jupyter/base-notebook:6d2a05346196 start.sh jupyter lab <span class="nt">--LabApp</span>.token<span class="o">=</span><span class="s1">''</span>
</code></pre></div></div>

<p>For jupyter classic:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run jupyter/base-notebook:6d2a05346196 start.sh jupyter notebook <span class="nt">--NotebookApp</span>.token<span class="o">=</span><span class="s1">''</span>
</code></pre></div></div>

<h2 id="enable-nbextension-spellchecker-for-markdown-or-any-other-nbextension">Enable nbextension spellchecker for markdown (or any other nbextension)</h2>

<p>NB: this works for classic notebooks only</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Update with your base image of choice</span>
<span class="k">FROM</span><span class="s"> jupyter/minimal-notebook:latest</span>

<span class="k">USER</span><span class="s"> $NB_USER</span>

<span class="k">RUN </span>pip <span class="nb">install </span>jupyter_contrib_nbextensions <span class="o">&amp;&amp;</span> <span class="se">\
</span>    jupyter contrib nbextension <span class="nb">install</span> <span class="nt">--user</span> <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="c"># can modify or enable additional extensions here</span>
    jupyter nbextension enable spellchecker/main --user
</code></pre></div></div>

<p>Ref:
<a href="https://github.com/jupyter/docker-stacks/issues/675">https://github.com/jupyter/docker-stacks/issues/675</a></p>

<h2 id="enable-auto-sklearn-notebooks">Enable auto-sklearn notebooks</h2>

<p>Using <code class="language-plaintext highlighter-rouge">auto-sklearn</code> requires <code class="language-plaintext highlighter-rouge">swig</code>, which the other notebook images lack, so it cant be experimented with. Also, there is no Conda package for <code class="language-plaintext highlighter-rouge">auto-sklearn</code>.</p>

<div class="language-dockerfile highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">ARG</span><span class="s"> BASE_CONTAINER=jupyter/scipy-notebook</span>
<span class="k">FROM</span><span class="s"> jupyter/scipy-notebook:latest</span>

<span class="k">USER</span><span class="s"> root</span>

<span class="c"># autosklearn requires swig, which no other image has</span>
<span class="k">RUN </span>apt-get update <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get <span class="nb">install</span> <span class="nt">-y</span> <span class="nt">--no-install-recommends</span> swig <span class="o">&amp;&amp;</span> <span class="se">\
</span>    apt-get clean <span class="o">&amp;&amp;</span> <span class="se">\
</span>    <span class="nb">rm</span> <span class="nt">-rf</span> /var/lib/apt/lists/<span class="k">*</span>


<span class="k">USER</span><span class="s"> $NB_UID</span>

<span class="k">RUN </span>pip <span class="nb">install</span> <span class="nt">--quiet</span> <span class="nt">--no-cache-dir</span> auto-sklearn
</code></pre></div></div>

  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Scaling Python ML</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Scaling Python ML</li><li><a class="u-email" href="mailto:holden@pigscanfly.ca">holden@pigscanfly.ca</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/scalingpythonml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">scalingpythonml</span></a></li><li><a href="https://www.twitter.com/holdenkarau"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">holdenkarau</span></a></li><li><a href="https://youtube.com/holdenkarau"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">holdenkarau</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Blog of my adventures working with different tools for scaling Python ML workloads.</p>
      </div>
    </div>

    <br />

    If you enjoy this work and have some extra $s, consider sponsoring <a href="https://github.com/sponsors/holdenk">my OSS work</a>.

    <!-- Disclamer -->

    This blog does not represent any of my employers, software projects, or foundations I'm a part of.
    I am one of the developers of Apache Spark <a href="https://amzn.to/2O6KYYH">and have some books published on the topic</a> <a href="https://amzn.to/2IA6Yf0">(plus a new Kubeflow book)</a> that may influence my views.
    Some of the links on this blog may generate an affiliate commission. I also earn royalties from my books.
    Generally speaking these do not cover the amount I spend on these adventures, but help defray my hardware, <a href="https://amzn.to/31mIOeK">coffee</a>, and <a href="https://dynamodonut.com/">artisinal doughnut</a> costs.

<!-- License -->
    <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />The posts on this blog are licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. <br />

<!-- Fork me on github -->
<a href="https://github.com/scalingpythonml/scalingpythonml.github.io" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  </div>

</footer>
</body>

</html>
