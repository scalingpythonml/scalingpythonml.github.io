<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Image Specifics | Scaling Python ML</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Image Specifics" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Blog of my adventures working with different tools for scaling Python ML workloads." />
<meta property="og:description" content="Blog of my adventures working with different tools for scaling Python ML workloads." />
<meta property="og:site_name" content="Scaling Python ML" />
<script type="application/ld+json">
{"url":"/subrepos/docker-stacks/docs/using/specifics.html","description":"Blog of my adventures working with different tools for scaling Python ML workloads.","headline":"Image Specifics","@type":"WebPage","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
  <link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Scaling Python ML" />

  <script>
  function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
  }
  window.onload = wrap_img;
  </script>

</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Scaling Python ML</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about.html">About</a><a class="page-link" href="/mailinglist.html">Mailinglist</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <header class="post-header">
    <h1 class="post-title">Image Specifics</h1>
  </header>

  <div class="post-content">
    <p>This page provides details about features specific to one or more images.</p>

<h2 id="apache-spark">Apache Sparkâ„¢</h2>

<h3 id="specific-docker-image-options">Specific Docker Image Options</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">-p 4040:4040</code> - The <code class="language-plaintext highlighter-rouge">jupyter/pyspark-notebook</code> and <code class="language-plaintext highlighter-rouge">jupyter/all-spark-notebook</code> images open <a href="http://spark.apache.org/docs/latest/monitoring.html">SparkUI (Spark Monitoring and Instrumentation UI)</a> at default port <code class="language-plaintext highlighter-rouge">4040</code>, this option map <code class="language-plaintext highlighter-rouge">4040</code> port inside docker container to <code class="language-plaintext highlighter-rouge">4040</code> port on host machine . Note every new spark context that is created is put onto an incrementing port (ie. 4040, 4041, 4042, etc.), and it might be necessary to open multiple ports. For example: <code class="language-plaintext highlighter-rouge">docker run -d -p 8888:8888 -p 4040:4040 -p 4041:4041 jupyter/pyspark-notebook</code>.</li>
</ul>

<h3 id="build-an-image-with-a-different-version-of-spark">Build an Image with a Different Version of Spark</h3>

<p>You can build a <code class="language-plaintext highlighter-rouge">pyspark-notebook</code> image (and also the downstream <code class="language-plaintext highlighter-rouge">all-spark-notebook</code> image) with a different version of Spark by overriding the default value of the following arguments at build time.</p>

<ul>
  <li>Spark distribution is defined by the combination of the Spark and the Hadoop version and verified by the package checksum, see <a href="https://spark.apache.org/downloads.html">Download Apache Spark</a> for more information. At this time the build will only work with the set of versions available on the Apache Spark download page, so it will not work with the archived versions.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">spark_version</code>: The Spark version to install (<code class="language-plaintext highlighter-rouge">3.0.0</code>).</li>
      <li><code class="language-plaintext highlighter-rouge">hadoop_version</code>: The Hadoop version (<code class="language-plaintext highlighter-rouge">3.2</code>).</li>
      <li><code class="language-plaintext highlighter-rouge">spark_checksum</code>: The package checksum (<code class="language-plaintext highlighter-rouge">BFE4540...</code>).</li>
    </ul>
  </li>
  <li>Spark is shipped with a version of Py4J that has to be referenced in the <code class="language-plaintext highlighter-rouge">PYTHONPATH</code>.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">py4j_version</code>: The Py4J version (<code class="language-plaintext highlighter-rouge">0.10.9</code>), see the tip below.</li>
    </ul>
  </li>
  <li>Spark can run with different OpenJDK versions.
    <ul>
      <li><code class="language-plaintext highlighter-rouge">openjdk_version</code>: The version of (JRE headless) the OpenJDK distribution (<code class="language-plaintext highlighter-rouge">11</code>), see <a href="https://packages.ubuntu.com/search?keywords=openjdk">Ubuntu packages</a>.</li>
    </ul>
  </li>
</ul>

<p>For example here is how to build a <code class="language-plaintext highlighter-rouge">pyspark-notebook</code> image with Spark <code class="language-plaintext highlighter-rouge">2.4.6</code>, Hadoop <code class="language-plaintext highlighter-rouge">2.7</code> and OpenJDK <code class="language-plaintext highlighter-rouge">8</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># From the root of the project</span>
<span class="c"># Build the image with different arguments</span>
docker build <span class="nt">--rm</span> <span class="nt">--force-rm</span> <span class="se">\</span>
    <span class="nt">-t</span> jupyter/pyspark-notebook:spark-2.4.6 ./pyspark-notebook <span class="se">\</span>
    <span class="nt">--build-arg</span> <span class="nv">spark_version</span><span class="o">=</span>2.4.6 <span class="se">\</span>
    <span class="nt">--build-arg</span> <span class="nv">hadoop_version</span><span class="o">=</span>2.7 <span class="se">\</span>
    <span class="nt">--build-arg</span> <span class="nv">spark_checksum</span><span class="o">=</span>3A9F401EDA9B5749CDAFD246B1D14219229C26387017791C345A23A65782FB8B25A302BF4AC1ED7C16A1FE83108E94E55DAD9639A51C751D81C8C0534A4A9641 <span class="se">\</span>
    <span class="nt">--build-arg</span> <span class="nv">openjdk_version</span><span class="o">=</span>8 <span class="se">\</span>
    <span class="nt">--build-arg</span> <span class="nv">py4j_version</span><span class="o">=</span>0.10.7

<span class="c"># Check the newly built image</span>
docker images jupyter/pyspark-notebook:spark-2.4.6

<span class="c"># REPOSITORY                 TAG                 IMAGE ID            CREATED             SIZE</span>
<span class="c"># jupyter/pyspark-notebook   spark-2.4.6         7ad7b5a9dbcd        4 minutes ago       3.44GB</span>

<span class="c"># Check the Spark version</span>
docker run <span class="nt">-it</span> <span class="nt">--rm</span> jupyter/pyspark-notebook:spark-2.4.6 pyspark <span class="nt">--version</span>

<span class="c"># Welcome to</span>
<span class="c">#       ____              __</span>
<span class="c">#      / __/__  ___ _____/ /__</span>
<span class="c">#     _\ \/ _ \/ _ `/ __/  '_/</span>
<span class="c">#    /___/ .__/\_,_/_/ /_/\_\   version 2.4.6</span>
<span class="c">#       /_/</span>
<span class="c">#                         </span>
<span class="c"># Using Scala version 2.11.12, OpenJDK 64-Bit Server VM, 1.8.0_265</span>
</code></pre></div></div>

<p><strong>Tip</strong>: to get the version of Py4J shipped with Spark:</p>

<ul>
  <li>Build a first image without changing <code class="language-plaintext highlighter-rouge">py4j_version</code> (it will not prevent the image to build it will just prevent Python to find the <code class="language-plaintext highlighter-rouge">pyspark</code> module),</li>
  <li>get the version (<code class="language-plaintext highlighter-rouge">ls /usr/local/spark/python/lib/</code>),</li>
  <li>set the version <code class="language-plaintext highlighter-rouge">--build-arg py4j_version=0.10.7</code>.</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">-it</span> <span class="nt">--rm</span> jupyter/pyspark-notebook:spark-2.4.6 <span class="nb">ls</span> /usr/local/spark/python/lib/ 
<span class="c"># py4j-0.10.7-src.zip  PY4J_LICENSE.txt  pyspark.zip</span>
<span class="c"># You can now set the build-arg</span>
<span class="c"># --build-arg py4j_version=</span>
</code></pre></div></div>

<p><em>Note: At the time of writing there is an issue preventing to use Spark <code class="language-plaintext highlighter-rouge">2.4.6</code> with Python <code class="language-plaintext highlighter-rouge">3.8</code>, see <a href="https://stackoverflow.com/a/62173969/4413446">this answer on SO</a> for more information.</em></p>

<h3 id="usage-examples">Usage Examples</h3>

<p>The <code class="language-plaintext highlighter-rouge">jupyter/pyspark-notebook</code> and <code class="language-plaintext highlighter-rouge">jupyter/all-spark-notebook</code> images support the use of <a href="https://spark.apache.org/">Apache Spark</a> in Python, R, and Scala notebooks. The following sections provide some examples of how to get started using them.</p>

<h4 id="using-spark-local-mode">Using Spark Local Mode</h4>

<p>Spark <strong>local mode</strong> is useful for experimentation on small data when you do not have a Spark cluster available.</p>

<h5 id="in-python">In Python</h5>

<p>In a Python notebook.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="c1"># Spark session &amp; context
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">'local'</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span>

<span class="c1"># Sum of the first 100 whole numbers
</span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">rdd</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="c1"># 5050
</span></code></pre></div></div>

<h5 id="in-r">In R</h5>

<p>In a R notebook with <a href="https://spark.apache.org/docs/latest/sparkr.html">SparkR</a>.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span><span class="w">

</span><span class="c1"># Spark session &amp; context</span><span class="w">
</span><span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparkR.session</span><span class="p">(</span><span class="s2">"local"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Sum of the first 100 whole numbers</span><span class="w">
</span><span class="n">sdf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">createDataFrame</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">))</span><span class="w">
</span><span class="n">dapplyCollect</span><span class="p">(</span><span class="n">sdf</span><span class="p">,</span><span class="w">
              </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> 
              </span><span class="p">{</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)}</span><span class="w">
             </span><span class="p">)</span><span class="w">
</span><span class="c1"># 5050</span><span class="w">
</span></code></pre></div></div>

<p>In a R notebook with <a href="https://spark.rstudio.com/">sparklyr</a>.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">sparklyr</span><span class="p">)</span><span class="w">

</span><span class="c1"># Spark configuration</span><span class="w">
</span><span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">spark_config</span><span class="p">()</span><span class="w">
</span><span class="c1"># Set the catalog implementation in-memory</span><span class="w">
</span><span class="n">conf</span><span class="o">$</span><span class="n">spark.sql.catalogImplementation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"in-memory"</span><span class="w">

</span><span class="c1"># Spark session &amp; context</span><span class="w">
</span><span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">spark_connect</span><span class="p">(</span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"local"</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">conf</span><span class="p">)</span><span class="w">

</span><span class="c1"># Sum of the first 100 whole numbers</span><span class="w">
</span><span class="n">sdf_len</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">repartition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
    </span><span class="n">spark_apply</span><span class="p">(</span><span class="k">function</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">e</span><span class="p">))</span><span class="w">
</span><span class="c1"># 5050</span><span class="w">
</span></code></pre></div></div>

<h5 id="in-scala">In Scala</h5>

<p>Spylon kernel instantiates a <code class="language-plaintext highlighter-rouge">SparkContext</code> for you in variable <code class="language-plaintext highlighter-rouge">sc</code> after you configure Spark
options in a <code class="language-plaintext highlighter-rouge">%%init_spark</code> magic cell.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">init_spark</span>
<span class="c1"># Configure Spark to use a local master
</span><span class="n">launcher</span><span class="p">.</span><span class="n">master</span> <span class="o">=</span> <span class="s">"local"</span>
</code></pre></div></div>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Sum of the first 100 whole numbers</span>
<span class="k">val</span> <span class="nv">rdd</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="mi">0</span> <span class="n">to</span> <span class="mi">100</span><span class="o">)</span>
<span class="nv">rdd</span><span class="o">.</span><span class="py">sum</span><span class="o">()</span>
<span class="c1">// 5050</span>
</code></pre></div></div>

<h4 id="connecting-to-a-spark-cluster-in-standalone-mode">Connecting to a Spark Cluster in Standalone Mode</h4>

<p>Connection to Spark Cluster on <strong><a href="https://spark.apache.org/docs/latest/spark-standalone.html">Standalone Mode</a></strong> requires the following set of steps:</p>

<ol>
  <li>Verify that the docker image (check the Dockerfile) and the Spark Cluster which is being
deployed, run the same version of Spark.</li>
  <li><a href="http://spark.apache.org/docs/latest/spark-standalone.html">Deploy Spark in Standalone Mode</a>.</li>
  <li>Run the Docker container with <code class="language-plaintext highlighter-rouge">--net=host</code> in a location that is network addressable by all of
your Spark workers. (This is a <a href="http://spark.apache.org/docs/latest/cluster-overview.html#components">Spark networking
requirement</a>.)
    <ul>
      <li>NOTE: When using <code class="language-plaintext highlighter-rouge">--net=host</code>, you must also use the flags <code class="language-plaintext highlighter-rouge">--pid=host -e
TINI_SUBREAPER=true</code>. See https://github.com/jupyter/docker-stacks/issues/64 for details.</li>
    </ul>
  </li>
</ol>

<p><strong>Note</strong>: In the following examples we are using the Spark master URL <code class="language-plaintext highlighter-rouge">spark://master:7077</code> that shall be replaced by the URL of the Spark master.</p>

<h5 id="in-python-1">In Python</h5>

<p>The <strong>same Python version</strong> need to be used on the notebook (where the driver is located) and on the Spark workers.
The python version used at driver and worker side can be adjusted by setting the environment variables <code class="language-plaintext highlighter-rouge">PYSPARK_PYTHON</code> and / or <code class="language-plaintext highlighter-rouge">PYSPARK_DRIVER_PYTHON</code>, see <a href="https://spark.apache.org/docs/latest/configuration.html">Spark Configuration</a> for more information.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SparkSession</span>

<span class="c1"># Spark session &amp; context
</span><span class="n">spark</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="p">.</span><span class="n">builder</span><span class="p">.</span><span class="n">master</span><span class="p">(</span><span class="s">'spark://master:7077'</span><span class="p">).</span><span class="n">getOrCreate</span><span class="p">()</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">spark</span><span class="p">.</span><span class="n">sparkContext</span>

<span class="c1"># Sum of the first 100 whole numbers
</span><span class="n">rdd</span> <span class="o">=</span> <span class="n">sc</span><span class="p">.</span><span class="n">parallelize</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">100</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
<span class="n">rdd</span><span class="p">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="c1"># 5050
</span></code></pre></div></div>

<h5 id="in-r-1">In R</h5>

<p>In a R notebook with <a href="https://spark.apache.org/docs/latest/sparkr.html">SparkR</a>.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">SparkR</span><span class="p">)</span><span class="w">

</span><span class="c1"># Spark session &amp; context</span><span class="w">
</span><span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sparkR.session</span><span class="p">(</span><span class="s2">"spark://master:7077"</span><span class="p">)</span><span class="w">

</span><span class="c1"># Sum of the first 100 whole numbers</span><span class="w">
</span><span class="n">sdf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">createDataFrame</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="m">1</span><span class="o">:</span><span class="m">100</span><span class="p">))</span><span class="w">
</span><span class="n">dapplyCollect</span><span class="p">(</span><span class="n">sdf</span><span class="p">,</span><span class="w">
              </span><span class="k">function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"> 
              </span><span class="p">{</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">x</span><span class="p">)}</span><span class="w">
             </span><span class="p">)</span><span class="w">
</span><span class="c1"># 5050</span><span class="w">
</span></code></pre></div></div>

<p>In a R notebook with <a href="https://spark.rstudio.com/">sparklyr</a>.</p>

<div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">sparklyr</span><span class="p">)</span><span class="w">

</span><span class="c1"># Spark session &amp; context</span><span class="w">
</span><span class="c1"># Spark configuration</span><span class="w">
</span><span class="n">conf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">spark_config</span><span class="p">()</span><span class="w">
</span><span class="c1"># Set the catalog implementation in-memory</span><span class="w">
</span><span class="n">conf</span><span class="o">$</span><span class="n">spark.sql.catalogImplementation</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="s2">"in-memory"</span><span class="w">
</span><span class="n">sc</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">spark_connect</span><span class="p">(</span><span class="n">master</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"spark://master:7077"</span><span class="p">,</span><span class="w"> </span><span class="n">config</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">conf</span><span class="p">)</span><span class="w">

</span><span class="c1"># Sum of the first 100 whole numbers</span><span class="w">
</span><span class="n">sdf_len</span><span class="p">(</span><span class="n">sc</span><span class="p">,</span><span class="w"> </span><span class="m">100</span><span class="p">,</span><span class="w"> </span><span class="n">repartition</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w"> </span><span class="o">%&gt;%</span><span class="w"> 
    </span><span class="n">spark_apply</span><span class="p">(</span><span class="k">function</span><span class="p">(</span><span class="n">e</span><span class="p">)</span><span class="w"> </span><span class="nf">sum</span><span class="p">(</span><span class="n">e</span><span class="p">))</span><span class="w">
</span><span class="c1"># 5050</span><span class="w">
</span></code></pre></div></div>

<h5 id="in-scala-1">In Scala</h5>

<p>Spylon kernel instantiates a <code class="language-plaintext highlighter-rouge">SparkContext</code> for you in variable <code class="language-plaintext highlighter-rouge">sc</code> after you configure Spark
options in a <code class="language-plaintext highlighter-rouge">%%init_spark</code> magic cell.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">%%</span><span class="n">init_spark</span>
<span class="c1"># Configure Spark to use a local master
</span><span class="n">launcher</span><span class="p">.</span><span class="n">master</span> <span class="o">=</span> <span class="s">"spark://master:7077"</span>
</code></pre></div></div>

<div class="language-scala highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Sum of the first 100 whole numbers</span>
<span class="k">val</span> <span class="nv">rdd</span> <span class="k">=</span> <span class="nv">sc</span><span class="o">.</span><span class="py">parallelize</span><span class="o">(</span><span class="mi">0</span> <span class="n">to</span> <span class="mi">100</span><span class="o">)</span>
<span class="nv">rdd</span><span class="o">.</span><span class="py">sum</span><span class="o">()</span>
<span class="c1">// 5050</span>
</code></pre></div></div>

<h2 id="tensorflow">Tensorflow</h2>

<p>The <code class="language-plaintext highlighter-rouge">jupyter/tensorflow-notebook</code> image supports the use of
<a href="https://www.tensorflow.org/">Tensorflow</a> in single machine or distributed mode.</p>

<h3 id="single-machine-mode">Single Machine Mode</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">hello</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="s">'Hello World!'</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">hello</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="distributed-mode">Distributed Mode</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">hello</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Variable</span><span class="p">(</span><span class="s">'Hello Distributed World!'</span><span class="p">)</span>

<span class="n">server</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">train</span><span class="p">.</span><span class="n">Server</span><span class="p">.</span><span class="n">create_local_server</span><span class="p">()</span>
<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">server</span><span class="p">.</span><span class="n">target</span><span class="p">)</span>
<span class="n">init</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">global_variables_initializer</span><span class="p">()</span>

<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">init</span><span class="p">)</span>
<span class="n">sess</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">hello</span><span class="p">)</span>
</code></pre></div></div>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Scaling Python ML</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Scaling Python ML</li><li><a class="u-email" href="mailto:holden@pigscanfly.ca">holden@pigscanfly.ca</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list">
  <li><a href="https://github.com/scalingpythonml"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">scalingpythonml</span></a></li><li><a href="https://www.twitter.com/holdenkarau"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">holdenkarau</span></a></li><li><a href="https://youtube.com/holdenkarau"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#youtube"></use></svg> <span class="username">holdenkarau</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Blog of my adventures working with different tools for scaling Python ML workloads.</p>
      </div>
    </div>

    <br />

    If you enjoy this work and have some extra $s, consider sponsoring <a href="https://github.com/sponsors/holdenk">my OSS work</a>.

    <!-- Disclamer -->

    This blog does not represent any of my employers, software projects, or foundations I'm a part of.
    I am one of the developers of Apache Spark <a href="https://amzn.to/2O6KYYH">and have some books published on the topic</a> <a href="https://amzn.to/2IA6Yf0">(plus a new Kubeflow book)</a> that may influence my views.
    Some of the links on this blog may generate an affiliate commission. I also earn royalties from my books.
    Generally speaking these do not cover the amount I spend on these adventures, but help defray my hardware, <a href="https://amzn.to/31mIOeK">coffee</a>, and <a href="https://dynamodonut.com/">artisinal doughnut</a> costs.

<!-- License -->
    <a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a><br />The posts on this blog are licensed under a <a rel="license" href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution 4.0 International License</a>. <br />

<!-- Fork me on github -->
<a href="https://github.com/scalingpythonml/scalingpythonml.github.io" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#151513; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

  </div>

</footer>
</body>

</html>
